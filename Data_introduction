Мои данные взяты из частного набора данных Шанхайского университета Цзяо Тонг в Китае, 
который можно загрузить и использовать после подачи заявки. 
Связь: https://bcmi.sjtu.edu.cn/home/seed/seed.html

Из библиотеки материалов были выбраны пятнадцать кинофрагментов из Китая (с позитивными, нейтральными и негативными эмоциями) в качестве стимулов для эксперимента. Критерии выбора фрагментов следующие: (a) общая продолжительность эксперимента не должна быть слишком долгой, чтобы избежать утомления испытуемых; (b) видео должно быть понятным без необходимости объяснений; (c) видео должно вызывать одну конкретную целевую эмоцию. Продолжительность каждого фрагмента составляет около 4 минут. Каждый фрагмент был тщательно смонтирован для создания последовательного эмоционального воздействия и максимизации эмоционального значения. Всего в эксперименте 15 испытаний. Перед каждым фрагментом есть 5 секунд подсказки, 45 секунд для самооценки, а после каждого фрагмента - 15 секунд отдыха. Порядок проигрывания  устроен таким образом, чтобы фрагменты с одинаковыми эмоциями не следовали друг за другом. Чтобы получить обратную связь, участников просят заполнить анкету сразу после просмотра каждого фрагмента и сообщить о своей эмоциональной реакции на каждый кинофрагмент.

"Китайская" папка содержит четыре подпапки.

01-EEG-raw: Содержит исходные сигналы ЭЭГ в формате .cnt с частотой дискретизации 1000 Гц.

02-EEG-DE-feature: Содержит признаки DE, извлеченные с помощью скользящего окна длиной 1 секунда и 4 секунды, а также исходный код для чтения данных.

03-Eye-tracking-excel: Содержит файлы Excel с данными о отслеживании взгляда.

04-Eye-tracking-feature: Содержит функции отслеживания взгляда в формате pickled и исходный код для чтения данных.

Папка "code" содержит исходный код моделей, используемых в этой статье:

- SVM, KNN и логистическая регрессия.
- Исходный код DNN.
- Традиционные методы слияния.
- Двухпиковый глубокий автокодировщик.
- Глубокий канонический анализ с механизмом внимания.

Исходный код также доступен на GitHub.

"information.xlsx" содержит информацию об эксперименте и испытуемых. Для видеофрагментов позитивные, негативные и нейтральные эмоции обозначены как 1, -1 и 0 соответственно. Для построения классификатора мы использовали 2, 0, 1 для обозначения позитивных, негативных и нейтральных эмоций соответственно (т.е. метки для видеофрагментов были увеличены на 1). Кроме того, мы не сохранили метки отслеживания взгляда, так как они совпадают с метками признаков DE ЭЭГ.















1. Wei-Long Zheng, and Bao-Liang Lu, Investigating Critical Frequency Bands and Channels for EEG-based Emotion Recognition with Deep Neural Networks, accepted by IEEE Transactions on Autonomous Mental Development (IEEE TAMD) 7(3): 162-175, 2015. [link] [BibTex]

2. Ruo-Nan Duan, Jia-Yi Zhu and Bao-Liang Lu, Differential Entropy Feature for EEG-based Emotion Classification, Proc. of the 6th International IEEE EMBS Conference on Neural Engineering (NER). 2013: 81-84. [link] [BibTex]
